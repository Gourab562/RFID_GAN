{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cox47pmTgHiH","outputId":"364c5a63-11ac-4a37-aa05-5ef96345c867"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4spHoPogJBe"},"outputs":[],"source":["rm -rf /DB\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ky02M05igJY3","outputId":"ece8d659-4dd9-4dcf-a70d-24a716700c0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  /content/drive/MyDrive/set_5.zip\n","  inflating: /DB/set_5.csv           \n"]}],"source":["!unzip '/content/drive/MyDrive/set_5.zip' -d '/DB'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wl8NIeQegJpf","outputId":"f8455ad6-e235-441b-e5b5-761485b66ff7","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["      index         0         1         2         3         4         5  \\\n","0         0 -41.05859 -45.22265 -48.47461 -45.05859 -43.61133 -42.35547   \n","1         1 -41.71875 -44.71289 -47.19140 -45.77148 -44.66210 -42.69531   \n","2         2 -44.21094 -50.81836 -60.81640 -47.71289 -44.87500 -43.44727   \n","3         3 -45.32812 -50.61718 -52.30273 -45.50977 -43.71094 -42.24609   \n","4         4 -44.56055 -48.62890 -52.46288 -45.85742 -43.56641 -42.42383   \n","...     ...       ...       ...       ...       ...       ...       ...   \n","1425    465 -48.99023 -46.34179 -44.43359 -44.37500 -46.00390 -48.68750   \n","1426    466 -48.53516 -46.49219 -44.67773 -43.86914 -44.97070 -47.18946   \n","1427    467 -51.35156 -48.44921 -46.46680 -46.43164 -48.39453 -52.57421   \n","1428    468 -56.33007 -51.62499 -48.89062 -47.68750 -49.25390 -52.49804   \n","1429    469 -55.02539 -50.90234 -47.97852 -47.61328 -49.94141 -53.94921   \n","\n","             6         7         8  ...      1591      1592      1593  \\\n","0    -40.64453 -40.85742 -42.40039  ... -60.81055 -60.11523 -62.47265   \n","1    -41.38476 -42.18163 -43.33008  ... -46.64063 -46.80469 -46.39844   \n","2    -42.53125 -43.33593 -45.64649  ... -48.11328 -48.77148 -48.52539   \n","3    -40.83789 -41.65625 -43.84570  ... -51.06835 -51.55663 -54.43749   \n","4    -41.15039 -41.82617 -43.71484  ... -48.54687 -49.12890 -49.18554   \n","...        ...       ...       ...  ...       ...       ...       ...   \n","1425 -51.18554 -50.62109 -52.62695  ... -50.74413 -49.61719 -49.57031   \n","1426 -51.29687 -53.51563 -55.00390  ... -53.27929 -54.14453 -54.46289   \n","1427 -59.37890 -58.65624 -58.68750  ... -54.00000 -54.42968 -54.57226   \n","1428 -60.39844 -73.67188 -67.64453  ... -52.70898 -52.82227 -51.79882   \n","1429 -64.60155 -66.87500 -63.56054  ... -49.62891 -49.27929 -48.61523   \n","\n","          1594      1595      1596      1597      1598      1599      1600  \n","0    -59.79882 -56.03906 -55.16602 -54.63672 -51.07226 -50.07617 -49.36523  \n","1    -46.28125 -46.27149 -46.21289 -46.14648 -46.70312 -46.02929 -46.94531  \n","2    -48.75195 -49.96679 -49.74218 -49.30859 -50.41406 -49.83007 -51.59765  \n","3    -54.77929 -57.38867 -56.53710 -57.20508 -58.11718 -60.58594 -65.33593  \n","4    -49.19335 -48.46874 -47.80273 -48.78906 -48.25000 -47.58985 -48.03515  \n","...        ...       ...       ...       ...       ...       ...       ...  \n","1425 -49.20703 -50.30468 -47.74804 -47.34961 -47.65429 -47.71680 -46.77930  \n","1426 -53.09765 -52.75781 -50.98437 -50.81445 -50.83984 -50.19531 -51.00780  \n","1427 -53.91602 -54.68554 -51.65429 -51.36132 -51.97460 -50.45116 -50.15038  \n","1428 -52.93359 -51.95507 -54.37695 -54.20898 -53.49609 -52.37109 -53.09375  \n","1429 -48.79296 -49.49804 -49.51562 -49.48632 -49.42187 -49.52343 -48.41797  \n","\n","[1430 rows x 1602 columns]\n"]}],"source":["import pandas as pd\n","df = pd.read_csv(\"/DB/set_5.csv\")\n","\n","# Select the last column of the DataFrame\n","last_column = df.iloc[:, -2]\n","X_train = df.iloc[:, :-2]\n","print(X_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2flUINhgJ5M","outputId":"003af0bb-44e0-4648-b343-0d7b85d5f8b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["         index         0         1         2         3         4         5  \\\n","0    -1.213399  1.184518  0.487298  0.016102  0.603862  0.930629  1.212474   \n","1    -1.209172  1.073694  0.565471  0.216777  0.492534  0.768071  1.159879   \n","2    -1.204946  0.655316 -0.370812 -1.913967  0.189355  0.735134  1.043503   \n","3    -1.200719  0.467768 -0.339960 -0.582558  0.533403  0.915219  1.229402   \n","4    -1.196493  0.596625 -0.035055 -0.607603  0.479113  0.937578  1.201895   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1425  0.751962 -0.147011  0.315677  0.648056  0.710614  0.560489  0.232502   \n","1426  0.756189 -0.070615  0.292613  0.609877  0.789611  0.720329  0.464345   \n","1427  0.760416 -0.543420 -0.007499  0.330093  0.389440  0.190650 -0.369022   \n","1428  0.764642 -1.379191 -0.494509 -0.048956  0.193320  0.057702 -0.357234   \n","1429  0.768869 -1.160167 -0.383690  0.093683  0.204911 -0.048658 -0.581823   \n","\n","             6         7         8  ...      1591      1592      1593  \\\n","0     1.476564  1.469629  1.261319  ... -1.457648 -1.333238 -1.596344   \n","1     1.364147  1.265114  1.108132  ...  0.108514  0.100793  0.142499   \n","2     1.190031  1.086840  0.726452  ... -0.054254 -0.111102 -0.087586   \n","3     1.447199  1.346255  1.023172  ... -0.380870 -0.411165 -0.727133   \n","4     1.399740  1.320012  1.044734  ... -0.102177 -0.149609 -0.158998   \n","...        ...       ...       ...  ...       ...       ...       ...   \n","1425 -0.124279 -0.038305 -0.423736  ... -0.345034 -0.202216 -0.200621   \n","1426 -0.141187 -0.485347 -0.815392  ... -0.625239 -0.689976 -0.729880   \n","1427 -1.368589 -1.279280 -1.422347  ... -0.704897 -0.720697 -0.741712   \n","1428 -1.523425 -3.598346 -2.898219  ... -0.562204 -0.547520 -0.441692   \n","1429 -2.161743 -2.548613 -2.225290  ... -0.221772 -0.165812 -0.097304   \n","\n","          1594      1595      1596      1597      1598      1599      1600  \n","0    -1.294211 -0.910197 -0.841743 -0.785129 -0.426468 -0.327171 -0.247074  \n","1     0.153249  0.147298  0.142509  0.141362  0.062440  0.125749  0.021092  \n","2    -0.111314 -0.252777 -0.245479 -0.203701 -0.352815 -0.299628 -0.494461  \n","3    -0.756720 -1.056313 -0.992471 -1.065399 -1.214796 -1.503407 -2.016881  \n","4    -0.158579 -0.090589 -0.032268 -0.147007 -0.110656 -0.048907 -0.099680  \n","...        ...       ...       ...       ...       ...       ...       ...  \n","1425 -0.160043 -0.289359 -0.026256  0.010071 -0.043996 -0.063115  0.039488  \n","1426 -0.576650 -0.554949 -0.382038 -0.368027 -0.400460 -0.340505 -0.429097  \n","1427 -0.664281 -0.763657 -0.455685 -0.427703 -0.527440 -0.369140 -0.334081  \n","1428 -0.559083 -0.468040 -0.754997 -0.738452 -0.697695 -0.584015 -0.660253  \n","1429 -0.115705 -0.202027 -0.220573 -0.223095 -0.241789 -0.265310 -0.142102  \n","\n","[1430 rows x 1602 columns]\n"]}],"source":["import numpy as np\n","\n","# Load your frequency response data into a variable, e.g., \"frequency_response_data\"\n","frequency_response_data = X_train\n","\n","# 1. Center the data\n","mean = np.mean(frequency_response_data)\n","frequency_response_data -= mean\n","\n","# 2. Scale the data\n","std_dev = np.std(frequency_response_data)\n","frequency_response_data /= std_dev\n","\n","# 3. Clip extreme values (optional)\n","# You can set a threshold to clip values if needed\n","# frequency_response_data = np.clip(frequency_response_data, min_threshold, max_threshold)\n","\n","# 4. Ensure the input range is appropriate for GAN activations\n","# If your GAN uses tanh activation in the generator, scale data to [-1, 1] range\n","# frequency_response_data *= 2  # Scale to [-1, 1]\n","\n","# Now, your frequency response data is centered, scaled, and potentially clipped, ready for GAN training.\n","print(frequency_response_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tYWTYZKiy1k0","outputId":"96f46177-9bbb-4da7-af3d-fd1c04378f12"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/100 | WGAN Loss: -5.577365875244141, -2.6925649642944336\n","Epoch 1/100 | WGAN Loss: -11.57784366607666, -4.318271636962891\n","Epoch 2/100 | WGAN Loss: -18.9161434173584, -7.951314926147461\n","Epoch 3/100 | WGAN Loss: -30.8344783782959, -11.437992095947266\n","Epoch 4/100 | WGAN Loss: -42.95418930053711, -10.586124420166016\n","Epoch 5/100 | WGAN Loss: -64.53857421875, -14.775724411010742\n","Epoch 6/100 | WGAN Loss: -85.39633178710938, -11.28681755065918\n","Epoch 7/100 | WGAN Loss: -118.59339904785156, -12.184846878051758\n","Epoch 8/100 | WGAN Loss: -143.0099334716797, -10.994701385498047\n","Epoch 9/100 | WGAN Loss: -186.54855346679688, -15.802230834960938\n","Epoch 10/100 | WGAN Loss: -230.53375244140625, -13.839656829833984\n","Epoch 11/100 | WGAN Loss: -278.4167785644531, -13.396098136901855\n","Epoch 12/100 | WGAN Loss: -322.46038818359375, -13.500579833984375\n","Epoch 13/100 | WGAN Loss: -387.966552734375, -15.187345504760742\n","Epoch 14/100 | WGAN Loss: -451.1109619140625, -10.356498718261719\n","Epoch 15/100 | WGAN Loss: -534.7433471679688, -15.802630424499512\n","Epoch 16/100 | WGAN Loss: -623.0927124023438, -11.16063404083252\n","Epoch 17/100 | WGAN Loss: -719.497802734375, -16.175128936767578\n","Epoch 18/100 | WGAN Loss: -805.0714111328125, -14.935214042663574\n","Epoch 19/100 | WGAN Loss: -919.9149169921875, -20.37952995300293\n","Epoch 20/100 | WGAN Loss: -950.7630615234375, -15.801496505737305\n","Epoch 21/100 | WGAN Loss: -1132.663330078125, -15.590192794799805\n","Epoch 22/100 | WGAN Loss: -1264.4825439453125, -12.906805038452148\n","Epoch 23/100 | WGAN Loss: -1382.3148193359375, -16.887924194335938\n","Epoch 24/100 | WGAN Loss: -1514.3846435546875, -14.552705764770508\n","Epoch 25/100 | WGAN Loss: -1703.658447265625, -16.066829681396484\n","Epoch 26/100 | WGAN Loss: -1864.739990234375, -15.606202125549316\n","Epoch 27/100 | WGAN Loss: -2001.83203125, -21.108251571655273\n","Epoch 28/100 | WGAN Loss: -2246.42333984375, -20.154296875\n","Epoch 29/100 | WGAN Loss: -2439.857421875, -17.678855895996094\n","Epoch 30/100 | WGAN Loss: -2610.837646484375, -18.121143341064453\n","Epoch 31/100 | WGAN Loss: -2709.044921875, -25.314807891845703\n","Epoch 32/100 | WGAN Loss: -3059.34765625, -21.050010681152344\n","Epoch 33/100 | WGAN Loss: -3328.53515625, -21.63091278076172\n","Epoch 34/100 | WGAN Loss: -3846.98388671875, -21.149938583374023\n","Epoch 35/100 | WGAN Loss: -3754.001708984375, -20.826580047607422\n","Epoch 36/100 | WGAN Loss: -4223.466796875, -16.357158660888672\n","Epoch 37/100 | WGAN Loss: -4465.26513671875, -27.79595184326172\n","Epoch 38/100 | WGAN Loss: -4812.078125, -26.185287475585938\n","Epoch 39/100 | WGAN Loss: -5196.5244140625, -25.163339614868164\n","Epoch 40/100 | WGAN Loss: -5576.05810546875, -22.81649398803711\n","Epoch 41/100 | WGAN Loss: -5921.748046875, -21.155900955200195\n","Epoch 42/100 | WGAN Loss: -6363.60546875, -24.861492156982422\n","Epoch 43/100 | WGAN Loss: -6996.73681640625, -30.519264221191406\n","Epoch 44/100 | WGAN Loss: -6915.21337890625, -26.002090454101562\n","Epoch 45/100 | WGAN Loss: -7568.23388671875, -32.72364044189453\n","Epoch 46/100 | WGAN Loss: -7782.4521484375, -30.579965591430664\n","Epoch 47/100 | WGAN Loss: -8121.21484375, -28.576555252075195\n","Epoch 48/100 | WGAN Loss: -8753.80078125, -30.59866714477539\n","Epoch 49/100 | WGAN Loss: -9217.259765625, -33.847923278808594\n","Epoch 50/100 | WGAN Loss: -10038.1240234375, -32.070255279541016\n","Epoch 51/100 | WGAN Loss: -10526.7109375, -34.532989501953125\n","Epoch 52/100 | WGAN Loss: -10347.8623046875, -29.444408416748047\n","Epoch 53/100 | WGAN Loss: -11392.181640625, -31.594942092895508\n","Epoch 54/100 | WGAN Loss: -12831.9765625, -38.05698776245117\n","Epoch 55/100 | WGAN Loss: -12712.86328125, -31.373384475708008\n","Epoch 56/100 | WGAN Loss: -13065.0263671875, -32.916404724121094\n","Epoch 57/100 | WGAN Loss: -14185.328125, -40.19541931152344\n","Epoch 58/100 | WGAN Loss: -14981.177734375, -42.15779113769531\n","Epoch 59/100 | WGAN Loss: -15799.111328125, -34.96196746826172\n","Epoch 60/100 | WGAN Loss: -16934.5390625, -40.084197998046875\n","Epoch 61/100 | WGAN Loss: -16915.580078125, -65.89337158203125\n","Epoch 62/100 | WGAN Loss: -18008.62890625, -41.810943603515625\n","Epoch 63/100 | WGAN Loss: -19126.90234375, -43.218780517578125\n","Epoch 64/100 | WGAN Loss: -19004.15625, -57.3751106262207\n","Epoch 65/100 | WGAN Loss: -20577.69921875, -42.10072708129883\n","Epoch 66/100 | WGAN Loss: -21200.17578125, -56.58025360107422\n","Epoch 67/100 | WGAN Loss: -21236.453125, -43.546287536621094\n","Epoch 68/100 | WGAN Loss: -23537.41015625, -48.22172546386719\n","Epoch 69/100 | WGAN Loss: -25269.478515625, -56.21709060668945\n","Epoch 70/100 | WGAN Loss: -25532.138671875, -46.76103973388672\n","Epoch 71/100 | WGAN Loss: -25642.25390625, -49.717369079589844\n","Epoch 72/100 | WGAN Loss: -27270.28515625, -62.50159454345703\n","Epoch 73/100 | WGAN Loss: -28457.693359375, -64.1463851928711\n","Epoch 74/100 | WGAN Loss: -29919.703125, -69.9805908203125\n","Epoch 75/100 | WGAN Loss: -31602.326171875, -75.68299102783203\n","Epoch 76/100 | WGAN Loss: -32376.828125, -58.005821228027344\n","Epoch 77/100 | WGAN Loss: -33133.92578125, -70.698486328125\n","Epoch 78/100 | WGAN Loss: -34517.6875, -61.155818939208984\n","Epoch 79/100 | WGAN Loss: -38195.73828125, -85.59834289550781\n","Epoch 80/100 | WGAN Loss: -38959.62890625, -73.38404846191406\n","Epoch 81/100 | WGAN Loss: -38168.63671875, -66.38785552978516\n","Epoch 82/100 | WGAN Loss: -42299.84375, -67.45370483398438\n","Epoch 83/100 | WGAN Loss: -42092.3359375, -97.17932891845703\n","Epoch 84/100 | WGAN Loss: -42634.921875, -81.98961639404297\n","Epoch 85/100 | WGAN Loss: -45524.15625, -74.35596466064453\n","Epoch 86/100 | WGAN Loss: -46678.2578125, -98.89534759521484\n","Epoch 87/100 | WGAN Loss: -48456.7265625, -87.97307586669922\n","Epoch 88/100 | WGAN Loss: -49734.4765625, -93.87150573730469\n","Epoch 89/100 | WGAN Loss: -53268.6953125, -87.12132263183594\n","Epoch 90/100 | WGAN Loss: -53862.12890625, -115.47804260253906\n","Epoch 91/100 | WGAN Loss: -56738.1484375, -108.87217712402344\n","Epoch 92/100 | WGAN Loss: -56410.796875, -120.28207397460938\n","Epoch 93/100 | WGAN Loss: -58534.7890625, -133.8837890625\n","Epoch 94/100 | WGAN Loss: -62797.88671875, -91.58834075927734\n","Epoch 95/100 | WGAN Loss: -62655.5390625, -115.47886657714844\n","Epoch 96/100 | WGAN Loss: -63770.3671875, -125.40563201904297\n","Epoch 97/100 | WGAN Loss: -67708.7890625, -158.76806640625\n","Epoch 98/100 | WGAN Loss: -72805.484375, -127.5841293334961\n","Epoch 99/100 | WGAN Loss: -71965.984375, -130.35992431640625\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n","from tensorflow import keras\n","from sklearn.preprocessing import StandardScaler\n","\n","# Assuming your preprocessed data is stored in a variable named \"frequency_response_data\"\n","data = frequency_response_data\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","data = scaler.fit_transform(data)\n","\n","# Define the WGAN architecture\n","latent_dim = 100  # Size of the random noise vector\n","\n","# Generator\n","generator = keras.Sequential([\n","    keras.layers.Dense(128, input_dim=latent_dim, activation='relu'),\n","    keras.layers.Dense(data.shape[1], activation='linear')\n","])\n","\n","# Critic (Discriminator for WGAN)\n","critic = keras.Sequential([\n","    keras.layers.Dense(64, input_dim=data.shape[1], activation='relu'),\n","    keras.layers.Dense(1, activation=None)  # No activation for WGAN\n","])\n","\n","# Define the WGAN model\n","gan_input = keras.Input(shape=(latent_dim,))\n","generated_data = generator(gan_input)\n","real_data = keras.Input(shape=(data.shape[1],))\n","critic_output_generated = critic(generated_data)\n","critic_output_real = critic(real_data)\n","\n","# Loss function\n","w_loss_gen = -tf.reduce_mean(critic_output_generated)\n","w_loss_real = -tf.reduce_mean(critic_output_real)\n","\n","# Combined WGAN model\n","wgan = keras.Model(inputs=[gan_input, real_data], outputs=[w_loss_gen, w_loss_real])\n","\n","# Optimizer\n","optimizer = keras.optimizers.RMSprop(learning_rate=0.00005)\n","\n","# Custom training loop\n","@tf.function\n","def train_step(real_data_batch, noise):\n","    with tf.GradientTape() as tape:\n","        generated_data_batch = generator(noise)\n","        critic_output_generated = critic(generated_data_batch)\n","        critic_output_real = critic(real_data_batch)\n","\n","        w_loss_gen = -tf.reduce_mean(critic_output_generated)\n","        w_loss_real = -tf.reduce_mean(critic_output_real)\n","\n","        total_loss = w_loss_gen + w_loss_real\n","\n","    gradients_wgan = tape.gradient(total_loss, wgan.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients_wgan, wgan.trainable_variables))\n","\n","    return w_loss_gen, w_loss_real\n","\n","# Training the WGAN\n","batch_size = 32\n","epochs = 100\n","\n","for epoch in range(epochs):\n","    for _ in range(len(data) // batch_size):\n","        idx = np.random.randint(0, len(data), batch_size)\n","        real_data_batch = data[idx]\n","        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n","\n","        w_loss_gen, w_loss_real = train_step(real_data_batch, noise)\n","\n","    # Print progress\n","    print(f\"Epoch {epoch}/{epochs} | WGAN Loss: {tf.keras.backend.eval(w_loss_gen)}, {tf.keras.backend.eval(w_loss_real)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6omA6eay1k1","outputId":"357ff271-83ad-4a99-a071-857e9fc4831a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/10 | WGAN Loss: -6.284127712249756, -6.229816436767578\n","Epoch 1/10 | WGAN Loss: -13.250825881958008, -10.739173889160156\n","Epoch 2/10 | WGAN Loss: -24.533100128173828, -13.315731048583984\n","Epoch 3/10 | WGAN Loss: -38.87641525268555, -11.428281784057617\n","Epoch 4/10 | WGAN Loss: -55.527313232421875, -16.83355140686035\n","Epoch 5/10 | WGAN Loss: -85.27204132080078, -23.583784103393555\n","Epoch 6/10 | WGAN Loss: -119.92345428466797, -19.37409019470215\n","Epoch 7/10 | WGAN Loss: -152.26739501953125, -18.80560874938965\n","Epoch 8/10 | WGAN Loss: -199.52828979492188, -18.874507904052734\n","Epoch 9/10 | WGAN Loss: -264.7488708496094, -21.15077018737793\n"]},{"name":"stderr","output_type":"stream","text":["/home/farhana/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:3504: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n","  \"Even though the tf.config.experimental_run_functions_eagerly \"\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.preprocessing import StandardScaler\n","\n","# Assuming your preprocessed data is stored in a variable named \"frequency_response_data\"\n","data = frequency_response_data\n","\n","# Normalize the data\n","scaler = StandardScaler()\n","data = scaler.fit_transform(data)\n","\n","# Define the WGAN architecture\n","latent_dim = 100  # Size of the random noise vector\n","\n","# Generator\n","generator = keras.Sequential([\n","    keras.layers.Dense(128, input_dim=latent_dim, activation='relu'),\n","    keras.layers.Dense(data.shape[1], activation='linear')\n","])\n","\n","# Critic (Discriminator for WGAN)\n","critic = keras.Sequential([\n","    keras.layers.Dense(64, input_dim=data.shape[1], activation='relu'),\n","    keras.layers.Dense(1, activation=None)  # No activation for WGAN\n","])\n","\n","# Define the WGAN model\n","gan_input = keras.Input(shape=(latent_dim,))\n","generated_data = generator(gan_input)\n","real_data = keras.Input(shape=(data.shape[1],))\n","critic_output_generated = critic(generated_data)\n","critic_output_real = critic(real_data)\n","\n","# Loss function\n","w_loss_gen = -tf.reduce_mean(critic_output_generated)\n","w_loss_real = -tf.reduce_mean(critic_output_real)\n","\n","# Combined WGAN model\n","wgan = keras.Model(inputs=[gan_input, real_data], outputs=[w_loss_gen, w_loss_real])\n","\n","# Optimizer\n","optimizer = keras.optimizers.RMSprop(learning_rate=0.00005)\n","\n","# Custom training loop\n","@tf.function\n","def train_step(real_data_batch, noise):\n","    with tf.GradientTape() as tape:\n","        generated_data_batch = generator(noise)\n","        critic_output_generated = critic(generated_data_batch)\n","        critic_output_real = critic(real_data_batch)\n","\n","        w_loss_gen = -tf.reduce_mean(critic_output_generated)\n","        w_loss_real = -tf.reduce_mean(critic_output_real)\n","\n","        total_loss = w_loss_gen + w_loss_real\n","\n","    gradients_wgan = tape.gradient(total_loss, wgan.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients_wgan, wgan.trainable_variables))\n","\n","    return w_loss_gen, w_loss_real\n","\n","# Training the WGAN\n","batch_size = 32\n","epochs = 10\n","\n","for epoch in range(epochs):\n","    for _ in range(len(data) // batch_size):\n","        idx = np.random.randint(0, len(data), batch_size)\n","        real_data_batch = data[idx]\n","        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n","\n","        w_loss_gen, w_loss_real = train_step(real_data_batch, noise)\n","\n","    # Print progress\n","    print(f\"Epoch {epoch}/{epochs} | WGAN Loss: {tf.keras.backend.eval(w_loss_gen)}, {tf.keras.backend.eval(w_loss_real)}\")\n","\n","# Anomaly Detection\n","def detect_anomalies_wgan(data):\n","    noise = np.random.normal(0, 1, (data.shape[0], latent_dim))\n","    generated_data = generator.predict(noise)\n","    critic_output = critic.predict(data)\n","    anomaly_scores = -critic_output  # Invert for anomaly detection\n","    return anomaly_scores\n","\n","# Detect anomalies in your data using the WGAN\n","anomaly_scores_wgan = detect_anomalies_wgan(data)\n","\n","# Set a threshold to classify anomalies\n","threshold_wgan = -0.5  # Adjust as needed\n","anomalies_wgan = anomaly_scores_wgan > threshold_wgan\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaLVZM1Wy1k1","outputId":"a4674272-5663-4ce3-e445-860ee4f1d2ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Indices of anomalies detected by WGAN:\n","(array([  39,  421,  423,  424,  458,  540,  767,  779,  788,  791,  792,\n","        793,  810,  818,  944,  946,  947,  949,  951,  956, 1171, 1181,\n","       1186, 1381, 1384]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0]))\n"]}],"source":["print(\"Indices of anomalies detected by WGAN:\")\n","print(np.where(anomalies_wgan))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z-Kl_BkVy1k2"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"nbformat":4,"nbformat_minor":0}